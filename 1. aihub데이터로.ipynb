{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262fbc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c575152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228468, 3)\n",
      "(28555, 3)\n"
     ]
    }
   ],
   "source": [
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv(\"dataset/aihub/train.csv\", sep=',', encoding='cp949')\n",
    "test = pd.read_csv(\"dataset/aihub/validation.csv\", sep=',', encoding='cp949')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955f700a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            아내가 드디어 출산하게 되어서 정말 신이 나.\n",
       "1              당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야.\n",
       "2              고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워.\n",
       "3        재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해.\n",
       "4                         빚을 드디어 다 갚게 되어서 이제야 안도감이 들어.\n",
       "5    이제 곧 은퇴할 시기가 되었어. 내가 먼저 은퇴를 하고 육 개월 후에 남편도 은퇴를...\n",
       "6                        사십 대에 접어들면서 머리카락이 많이 빠져 고민이야.\n",
       "7                                       이제 돈이라면 지긋지긋해.\n",
       "8               친구들이 나를 괴롭혀. 부모님과 선생님께 얘기했는데도 믿어주지 않아.\n",
       "9                                         친구 때문에 눈물 나.\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 문장 추출\n",
    "sentences = train['sentence']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b737d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['id']\n",
    "del test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eefab9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 아내가 드디어 출산하게 되어서 정말 신이 나. [SEP]',\n",
       " '[CLS] 당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야. [SEP]',\n",
       " '[CLS] 고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워. [SEP]',\n",
       " '[CLS] 재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해. [SEP]',\n",
       " '[CLS] 빚을 드디어 다 갚게 되어서 이제야 안도감이 들어. [SEP]',\n",
       " '[CLS] 이제 곧 은퇴할 시기가 되었어. 내가 먼저 은퇴를 하고 육 개월 후에 남편도 은퇴를 하고. 그러면 둘이 하루 종일 함께 있겠네. 생각만 해도 숨이 막혀. [SEP]',\n",
       " '[CLS] 사십 대에 접어들면서 머리카락이 많이 빠져 고민이야. [SEP]',\n",
       " '[CLS] 이제 돈이라면 지긋지긋해. [SEP]',\n",
       " '[CLS] 친구들이 나를 괴롭혀. 부모님과 선생님께 얘기했는데도 믿어주지 않아. [SEP]',\n",
       " '[CLS] 친구 때문에 눈물 나. [SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT의 입력 형식에 맞게 변환\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0fbe1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨 추출\n",
    "labels = train['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72a85245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 아내가 드디어 출산하게 되어서 정말 신이 나. [SEP]\n",
      "['[CLS]', '아', '##내', '##가', '드', '##디', '##어', '출', '##산', '##하게', '되어', '##서', '정', '##말', '신', '##이', '나', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "747d780b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  9519, 31605, 11287,  9113, 48446, 12965,  9768, 21386,\n",
       "       17594, 37909, 12424,  9670, 89523,  9487, 10739,  8982,   119,\n",
       "         102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 토큰의 최대 시퀀스 길이\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 토큰을 숫자 인덱스로 변환\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c49f1f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "769d8eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   101,   9781,  17196,  61688,   8888,  23811,  90373,   9934,  30842,\n",
      "         12453,   9495, 119185,  14040,  17360,  48549,    119,    102,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(1)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "tensor([   101,   8982,  80001,  79633,   9376,  42771,  11489,   9576,  69047,\n",
      "          9954,   8867,   9356,  25503,   9709,  75855,  11287,   9682, 100585,\n",
      "         28911,   9604,  12692,   9798, 119412,  10892,   9487, 119439,  29935,\n",
      "         88904,  79633,   8895,  11467,   8843,  62675,   9965,  12965,    119,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(0)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 훈련셋과 검증셋으로 분리\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels, \n",
    "                                                                                    random_state=2018, \n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=2018, \n",
    "                                                       test_size=0.1)\n",
    "\n",
    "# 데이터를 파이토치의 텐서로 변환\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\t\t\t\t\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])\n",
    "print(train_masks[0])\n",
    "print(validation_inputs[0])\n",
    "print(validation_labels[0])\n",
    "print(validation_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c969d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈\n",
    "batch_size = 16\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dd724ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     요즘 부모님과 많이 부딪혀.\n",
       "1                            엄마가 결국 집을 나갔어. 너무 너무 슬퍼.\n",
       "2                      학교에서 한 친구를 괴롭히는 무리에게 그만하라고 했어.\n",
       "3    이번에 팀장님이 간단한 조사 업무를 부탁하셨는데 내가 잘못 처리했어. 너무 절망적이야.\n",
       "4                   남편이 이혼할 때 위자료를 주지 않으려고 변호사를 고용했어.\n",
       "5         친구들과 노후에 대한 이야기를 하다 보니 서로 노후 자금 차이가 상당히 컸어.\n",
       "6                             직장에서 모함을 받았어. 난 정말 억울해.\n",
       "7                      요즘 딸에게 뭔가 물어보면 신경질부터 내는데 화가 나.\n",
       "8              언제까지 대출금을 갚으며 살아야 할까? 포기하고 싶어지고 너무 슬퍼.\n",
       "9                           이 업무는 하루 만에 끝낼 수 있을 것 같아.\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 문장 추출\n",
    "sentences = test['sentence']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe0b2f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 요즘 부모님과 많이 부딪혀. [SEP]',\n",
       " '[CLS] 엄마가 결국 집을 나갔어. 너무 너무 슬퍼. [SEP]',\n",
       " '[CLS] 학교에서 한 친구를 괴롭히는 무리에게 그만하라고 했어. [SEP]',\n",
       " '[CLS] 이번에 팀장님이 간단한 조사 업무를 부탁하셨는데 내가 잘못 처리했어. 너무 절망적이야. [SEP]',\n",
       " '[CLS] 남편이 이혼할 때 위자료를 주지 않으려고 변호사를 고용했어. [SEP]',\n",
       " '[CLS] 친구들과 노후에 대한 이야기를 하다 보니 서로 노후 자금 차이가 상당히 컸어. [SEP]',\n",
       " '[CLS] 직장에서 모함을 받았어. 난 정말 억울해. [SEP]',\n",
       " '[CLS] 요즘 딸에게 뭔가 물어보면 신경질부터 내는데 화가 나. [SEP]',\n",
       " '[CLS] 언제까지 대출금을 갚으며 살아야 할까? 포기하고 싶어지고 너무 슬퍼. [SEP]',\n",
       " '[CLS] 이 업무는 하루 만에 끝낼 수 있을 것 같아. [SEP]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT의 입력 형식에 맞게 변환\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e87051fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨 추출\n",
    "labels = test['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91b3a9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 요즘 부모님과 많이 부딪혀. [SEP]\n",
      "['[CLS]', '요', '##즘', '부', '##모', '##님', '##과', '많이', '부', '##딪', '##혀', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "675ac1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   9599, 119229,   9365,  39420, 108578,  11882,  47058,\n",
       "         9365, 118824,  80579,    119,    102,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 토큰의 최대 시퀀스 길이\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 토큰을 숫자 인덱스로 변환\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b39469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a03b8704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   101,   9599, 119229,   9365,  39420, 108578,  11882,  47058,   9365,\n",
      "        118824,  80579,    119,    102,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(0)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 파이토치의 텐서로 변환\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "print(test_inputs[0])\n",
    "print(test_labels[0])\n",
    "print(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab92c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈\n",
    "batch_size = 16\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "909928b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPU 디바이스 이름 구함\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# GPU 디바이스 이름 검사\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60db13ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5f7ae83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류를 위한 BERT 모델 생성\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a634132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # 학습률\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "\n",
    "# 에폭수\n",
    "epochs = 4\n",
    "\n",
    "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b32381b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "872b97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bb1417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   500  of  12,852.    Elapsed: 0:01:49.\n",
      "  Batch 1,000  of  12,852.    Elapsed: 0:03:38.\n",
      "  Batch 1,500  of  12,852.    Elapsed: 0:05:27.\n",
      "  Batch 2,000  of  12,852.    Elapsed: 0:07:15.\n",
      "  Batch 2,500  of  12,852.    Elapsed: 0:09:04.\n",
      "  Batch 3,000  of  12,852.    Elapsed: 0:10:53.\n",
      "  Batch 3,500  of  12,852.    Elapsed: 0:12:42.\n",
      "  Batch 4,000  of  12,852.    Elapsed: 0:14:31.\n",
      "  Batch 4,500  of  12,852.    Elapsed: 0:16:20.\n",
      "  Batch 5,000  of  12,852.    Elapsed: 0:18:09.\n",
      "  Batch 5,500  of  12,852.    Elapsed: 0:19:57.\n",
      "  Batch 6,000  of  12,852.    Elapsed: 0:21:46.\n",
      "  Batch 6,500  of  12,852.    Elapsed: 0:23:35.\n",
      "  Batch 7,000  of  12,852.    Elapsed: 0:25:24.\n",
      "  Batch 7,500  of  12,852.    Elapsed: 0:27:13.\n",
      "  Batch 8,000  of  12,852.    Elapsed: 0:29:02.\n",
      "  Batch 8,500  of  12,852.    Elapsed: 0:30:50.\n",
      "  Batch 9,000  of  12,852.    Elapsed: 0:32:39.\n",
      "  Batch 9,500  of  12,852.    Elapsed: 0:34:28.\n",
      "  Batch 10,000  of  12,852.    Elapsed: 0:36:17.\n",
      "  Batch 10,500  of  12,852.    Elapsed: 0:38:06.\n",
      "  Batch 11,000  of  12,852.    Elapsed: 0:39:55.\n",
      "  Batch 11,500  of  12,852.    Elapsed: 0:41:43.\n",
      "  Batch 12,000  of  12,852.    Elapsed: 0:43:32.\n",
      "  Batch 12,500  of  12,852.    Elapsed: 0:45:21.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epcoh took: 0:46:38\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.92\n",
      "  Validation took: 0:01:21\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  12,852.    Elapsed: 0:01:49.\n",
      "  Batch 1,000  of  12,852.    Elapsed: 0:03:38.\n",
      "  Batch 1,500  of  12,852.    Elapsed: 0:05:27.\n",
      "  Batch 2,000  of  12,852.    Elapsed: 0:07:15.\n",
      "  Batch 2,500  of  12,852.    Elapsed: 0:09:04.\n",
      "  Batch 3,000  of  12,852.    Elapsed: 0:10:53.\n",
      "  Batch 3,500  of  12,852.    Elapsed: 0:12:42.\n",
      "  Batch 4,000  of  12,852.    Elapsed: 0:14:30.\n",
      "  Batch 4,500  of  12,852.    Elapsed: 0:16:19.\n",
      "  Batch 5,000  of  12,852.    Elapsed: 0:18:08.\n",
      "  Batch 5,500  of  12,852.    Elapsed: 0:19:57.\n",
      "  Batch 6,000  of  12,852.    Elapsed: 0:21:45.\n",
      "  Batch 6,500  of  12,852.    Elapsed: 0:23:34.\n",
      "  Batch 7,000  of  12,852.    Elapsed: 0:25:23.\n",
      "  Batch 7,500  of  12,852.    Elapsed: 0:27:12.\n",
      "  Batch 8,000  of  12,852.    Elapsed: 0:29:00.\n",
      "  Batch 8,500  of  12,852.    Elapsed: 0:30:49.\n",
      "  Batch 9,000  of  12,852.    Elapsed: 0:32:38.\n",
      "  Batch 9,500  of  12,852.    Elapsed: 0:34:27.\n",
      "  Batch 10,000  of  12,852.    Elapsed: 0:36:15.\n",
      "  Batch 10,500  of  12,852.    Elapsed: 0:38:04.\n",
      "  Batch 11,000  of  12,852.    Elapsed: 0:39:53.\n",
      "  Batch 11,500  of  12,852.    Elapsed: 0:41:41.\n",
      "  Batch 12,000  of  12,852.    Elapsed: 0:43:30.\n",
      "  Batch 12,500  of  12,852.    Elapsed: 0:45:19.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epcoh took: 0:46:35\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation took: 0:01:21\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  12,852.    Elapsed: 0:01:49.\n",
      "  Batch 1,000  of  12,852.    Elapsed: 0:03:37.\n",
      "  Batch 1,500  of  12,852.    Elapsed: 0:05:26.\n",
      "  Batch 2,000  of  12,852.    Elapsed: 0:07:14.\n",
      "  Batch 2,500  of  12,852.    Elapsed: 0:09:03.\n",
      "  Batch 3,000  of  12,852.    Elapsed: 0:10:51.\n",
      "  Batch 3,500  of  12,852.    Elapsed: 0:12:40.\n",
      "  Batch 4,000  of  12,852.    Elapsed: 0:14:29.\n",
      "  Batch 4,500  of  12,852.    Elapsed: 0:16:18.\n",
      "  Batch 5,000  of  12,852.    Elapsed: 0:18:06.\n",
      "  Batch 5,500  of  12,852.    Elapsed: 0:19:54.\n",
      "  Batch 6,000  of  12,852.    Elapsed: 0:21:43.\n",
      "  Batch 6,500  of  12,852.    Elapsed: 0:23:32.\n",
      "  Batch 7,000  of  12,852.    Elapsed: 0:25:20.\n",
      "  Batch 7,500  of  12,852.    Elapsed: 0:27:08.\n",
      "  Batch 8,000  of  12,852.    Elapsed: 0:28:57.\n",
      "  Batch 8,500  of  12,852.    Elapsed: 0:30:46.\n",
      "  Batch 9,000  of  12,852.    Elapsed: 0:32:34.\n",
      "  Batch 9,500  of  12,852.    Elapsed: 0:34:23.\n",
      "  Batch 10,000  of  12,852.    Elapsed: 0:36:11.\n",
      "  Batch 10,500  of  12,852.    Elapsed: 0:38:00.\n",
      "  Batch 11,000  of  12,852.    Elapsed: 0:39:48.\n",
      "  Batch 11,500  of  12,852.    Elapsed: 0:41:37.\n",
      "  Batch 12,000  of  12,852.    Elapsed: 0:43:25.\n",
      "  Batch 12,500  of  12,852.    Elapsed: 0:45:14.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epcoh took: 0:46:30\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:01:21\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  12,852.    Elapsed: 0:01:50.\n",
      "  Batch 1,000  of  12,852.    Elapsed: 0:03:39.\n",
      "  Batch 1,500  of  12,852.    Elapsed: 0:05:28.\n",
      "  Batch 2,000  of  12,852.    Elapsed: 0:07:16.\n",
      "  Batch 2,500  of  12,852.    Elapsed: 0:09:05.\n",
      "  Batch 3,000  of  12,852.    Elapsed: 0:10:53.\n",
      "  Batch 3,500  of  12,852.    Elapsed: 0:12:41.\n",
      "  Batch 4,000  of  12,852.    Elapsed: 0:14:30.\n",
      "  Batch 4,500  of  12,852.    Elapsed: 0:16:18.\n",
      "  Batch 5,000  of  12,852.    Elapsed: 0:18:06.\n",
      "  Batch 5,500  of  12,852.    Elapsed: 0:19:55.\n",
      "  Batch 6,000  of  12,852.    Elapsed: 0:21:43.\n",
      "  Batch 6,500  of  12,852.    Elapsed: 0:23:32.\n",
      "  Batch 7,000  of  12,852.    Elapsed: 0:25:20.\n",
      "  Batch 7,500  of  12,852.    Elapsed: 0:27:08.\n",
      "  Batch 8,000  of  12,852.    Elapsed: 0:28:57.\n",
      "  Batch 8,500  of  12,852.    Elapsed: 0:30:45.\n",
      "  Batch 9,000  of  12,852.    Elapsed: 0:32:34.\n",
      "  Batch 9,500  of  12,852.    Elapsed: 0:34:22.\n",
      "  Batch 10,000  of  12,852.    Elapsed: 0:36:10.\n",
      "  Batch 10,500  of  12,852.    Elapsed: 0:37:59.\n",
      "  Batch 11,000  of  12,852.    Elapsed: 0:39:47.\n",
      "  Batch 11,500  of  12,852.    Elapsed: 0:41:35.\n",
      "  Batch 12,000  of  12,852.    Elapsed: 0:43:24.\n",
      "  Batch 12,500  of  12,852.    Elapsed: 0:45:12.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:46:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:01:21\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "       \n",
    "        # Forward 수행                \n",
    "        outputs = model(torch.tensor(b_input_ids).to(device).long(), \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(torch.tensor(b_input_ids).to(device).long(), \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # 로스 구함\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25752944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,785.    Elapsed: 0:00:06.\n",
      "  Batch   200  of  1,785.    Elapsed: 0:00:11.\n",
      "  Batch   300  of  1,785.    Elapsed: 0:00:17.\n",
      "  Batch   400  of  1,785.    Elapsed: 0:00:23.\n",
      "  Batch   500  of  1,785.    Elapsed: 0:00:28.\n",
      "  Batch   600  of  1,785.    Elapsed: 0:00:34.\n",
      "  Batch   700  of  1,785.    Elapsed: 0:00:40.\n",
      "  Batch   800  of  1,785.    Elapsed: 0:00:45.\n",
      "  Batch   900  of  1,785.    Elapsed: 0:00:51.\n",
      "  Batch 1,000  of  1,785.    Elapsed: 0:00:57.\n",
      "  Batch 1,100  of  1,785.    Elapsed: 0:01:02.\n",
      "  Batch 1,200  of  1,785.    Elapsed: 0:01:08.\n",
      "  Batch 1,300  of  1,785.    Elapsed: 0:01:14.\n",
      "  Batch 1,400  of  1,785.    Elapsed: 0:01:19.\n",
      "  Batch 1,500  of  1,785.    Elapsed: 0:01:25.\n",
      "  Batch 1,600  of  1,785.    Elapsed: 0:01:31.\n",
      "  Batch 1,700  of  1,785.    Elapsed: 0:01:37.\n",
      "\n",
      "Accuracy: 0.92\n",
      "Test took: 0:01:41\n"
     ]
    }
   ],
   "source": [
    "#시작 시간 설정\n",
    "t0 = time.time()\n",
    "\n",
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "# 변수 초기화\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # 경과 정보 표시\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # 배치를 GPU에 넣음\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # 배치에서 데이터 추출\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(torch.tensor(b_input_ids).to(device).long(),\n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9a319fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 변환\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # 입력 토큰의 최대 시퀀스 길이\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # 토큰을 숫자 인덱스로 변환\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # 어텐션 마스크 초기화\n",
    "    attention_masks = []\n",
    "\n",
    "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # 데이터를 파이토치의 텐서로 변환\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac628d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 테스트\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 문장을 입력 데이터로 변환\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "\n",
    "    # 데이터를 GPU에 넣음\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "            \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(torch.tensor(b_input_ids).to(device).long(), \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0510ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.97584164  0.6854355 ]]\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['호잉이 이 감독의 기대에 부응할 때, KT도 더 높은 곳으로 비상할 수 있다.'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a593575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.374845 -2.615273]]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['뒷문이 흔들린 롯데는 전반기를 32승1무44패(승률 0.421)로 마쳤다.'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06829ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.6330535  1.5442894]]\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['그러나 로맥은 언제 찾아올지 모를 기회에 대비해 꾸준히 준비했다. '])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33827eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.90503025 -0.7276854 ]]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['절호의 찬스 놓친 삼성 원태인'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c91fa484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9183775 -2.0392318]]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['투구 맞은 이원석, 교체 후 병원 후송 \"팔꿈치 정밀검진 예정\"'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c107520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.3128585  1.0872172]]\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['8일 인천 LG전서 좌월 역전 결승 투런포를 뽑아냈다.'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4255b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0438426 -2.2086687]]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeepLearning_1\\anaconda3\\envs\\windup\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['올 시즌 극심한 부진으로 후반기 초반 2군에 다녀왔고, 1군에 돌아오니 최항과 최주환의 맹활약으로 출전기회가 사실상 막혔다. 대타와 지명타자 등을 오가며 불규칙으로 나가다 보니 더더욱 타격감을 찾기 어려웠다.'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7c223d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('posnega_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a29ce2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('posnega_model\\\\tokenizer_config.json',\n",
       " 'posnega_model\\\\special_tokens_map.json',\n",
       " 'posnega_model\\\\vocab.txt',\n",
       " 'posnega_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('posnega_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a31050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "windup",
   "language": "python",
   "name": "windup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
